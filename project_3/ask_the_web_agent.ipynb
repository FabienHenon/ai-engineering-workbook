{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‑the‑Web Agent**\n",
    "\n",
    "Welcome to Project 3! In this project, you will learn how to use tool‑calling LLMs, extend them with custom tools, and build a simplified *Perplexity‑style* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‑calling capability\n",
    "* Combine LLM with a web‑search tool to build a simple ask‑the‑web agent.\n",
    "* Connect to external tools using **MCP (Model Context Protocol)**, a universal standard for LLM‑tool integration.\n",
    "* Optionally build a UI using Chainlit to test your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "0. Environment setup\n",
    "1. Write simple tools and connect them to an LLM\n",
    "2. Standardize tool calling with JSON schemas\n",
    "3. Use LangGraph for tool calling\n",
    "4. Build a Perplexity-style web-search agent\n",
    "5. (Optional) MCP: connect to external tool servers\n",
    "6. (Optional) A minimal UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "```\n",
    "\n",
    "#### Option 2: UV (faster)\n",
    "\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create venv and install dependencies\n",
    "uv venv .venv-web-agent-uv && source .venv-web-agent-uv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Now open your notebook and switch to the `web_agent` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Set up Ollama\n",
    "\n",
    "In this project, we use **Ollama** to load and use open-weight LLMs. We start with smaller models like `gemma3:1b` and then switch to larger models like `llama3.2:3b`.\n",
    "\n",
    "Start the **Ollama** server in a terminal. This launches a local API endpoint that listens for LLM requests.\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Downloads the model so you can run them locally without API calls. \n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "You can explore other available models [here](https://ollama.com/library) and pull them to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bejkbbzrdy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running. Installed models: ['llama3.2:3b', 'gemma3:1b']\n"
     ]
    }
   ],
   "source": [
    "# Quick check: is Ollama running?\n",
    "# If this fails, open a terminal and run: ollama serve\n",
    "\n",
    "import httpx\n",
    "\n",
    "response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Ollama is running. Installed models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 1- Tool Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "<img src=\"assets/tools.png\" width=\"700\">\n",
    "\n",
    "As show below, We first implement a tool, then describe the tool as part of the model's prompt. When the model decides that a tool is needed, it emits a structured output. A parser will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "<img src=\"assets/tool_flow.png\" width=\"700\">\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3:1b` model to use it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23°C and sunny in San Francisco.\")\n",
    "#\n",
    "# Output:\n",
    "#   • Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") → \"It is 23°C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "def get_current_weather(location: str) -> str:\n",
    "    return f\"It is 23°C and sunny in {location}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Step 2: Create a prompt to teach the LLM when and how to use your tool\n",
    "# ----------------------------------------------------------------------\n",
    "# What to include:\n",
    "#   • A SYSTEM_PROMPT that tells the model about the tool use and describes the tool\n",
    "#   • A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the weather. You have access to the following tool:\n",
    "Tool: get_current_weather(location)\n",
    "Description: This tool takes a location as input and returns a short, human-readable sentence describing the current weather in that location.\n",
    "To use the tool, respond with a message in the following format:\n",
    "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"location\": \"<location_name>\"}}\n",
    "\"\"\"\n",
    "\n",
    "USER_QUESTION = \"What is the weather in San Diego today?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: model='gemma3:1b' created_at='2026-02-06T15:35:23.133641Z' done=True done_reason='stop' total_duration=3541010000 load_duration=201637041 prompt_eval_count=125 prompt_eval_duration=1566408792 eval_count=26 eval_duration=1752033835 message=Message(role='assistant', content='TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"location\": \"San Diego\"}}\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"location\": \"San Diego\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Create an Ollama client\n",
    "#   2. Use chat.completions.create to send your prompt to gemma3:1b\n",
    "#   3. Print the response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:1b', messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "])\n",
    "print(\"LLM response:\", response)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: It is 23°C and sunny in San Diego.\n",
      "Done. It is 23°C and sunny in San Diego.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Manually parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23°C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "tool_call_pattern = r\"TOOL_CALL:\\s*(\\{.*\\})\"\n",
    "\n",
    "def call_tool(response_content: str):\n",
    "  match = re.search(tool_call_pattern, response_content)\n",
    "  if match:\n",
    "      tool_call_json = match.group(1)\n",
    "      tool_call = json.loads(tool_call_json)\n",
    "      tool_name = tool_call['name']\n",
    "      tool_args = tool_call['args']\n",
    "      \n",
    "      if tool_name == \"get_current_weather\":\n",
    "          return get_current_weather(**tool_args)\n",
    "  else:\n",
    "      return None\n",
    "\n",
    "res = call_tool(response['message']['content'])\n",
    "print(\"Result:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 2- Standardize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing a function, manually teaching the LLM about it, and write a regex to parse the output. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function's signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Get the current weather for a given city.',\n",
      " 'name': 'get_current_weather',\n",
      " 'parameters': [{'description': 'The city to get the weather for.',\n",
      "                 'name': 'city',\n",
      "                 'type': 'str'},\n",
      "                {'description': 'The unit of temperature. Defaults to '\n",
      "                                '\"celsius\".',\n",
      "                 'name': 'unit',\n",
      "                 'type': 'str'}]}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Rewrite the get_current_weather function with docstring and arg types\n",
    "#   2. Use `inspect.signature` to automatically get function parameters and docstring\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing: name, description, and parameters.\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The city to get the weather for.\n",
    "        unit (str, optional): The unit of temperature. Defaults to \"celsius\".\n",
    "    \n",
    "    Returns:\n",
    "        str: A string describing the current weather.\n",
    "    \"\"\"\n",
    "    return f\"It is 23°C and sunny in {city}.\"\n",
    "\n",
    "def to_schema(fn):\n",
    "    sig = inspect.signature(fn)\n",
    "    doc = inspect.getdoc(fn)\n",
    "    \n",
    "    args_section = re.search(r\"Args:\\n(.*?)(?:\\n\\n|$)\", doc, re.S)\n",
    "    lines = [ln.strip() for ln in args_section.group(1).splitlines() if ln.strip()]\n",
    "    descriptions = {n.split(\" \")[0]: d.strip() for n, d in (ln.split(\":\", 1) for ln in lines)}\n",
    "        \n",
    "    data = {\n",
    "        \"name\": fn.__name__,\n",
    "        \"description\": doc.split(\"\\n\")[0] if doc else \"\",\n",
    "        \"parameters\": [{\"name\": n, \"type\": str(t.annotation.__name__), \"description\": descriptions.get(n, \"\")} for (n, t) in sig.parameters.items()]\n",
    "    }\n",
    "    return data\n",
    "\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"get_current_weather\",\n",
      "  \"description\": \"Get the current weather for a given city.\",\n",
      "  \"parameters\": [\n",
      "    {\n",
      "      \"name\": \"city\",\n",
      "      \"type\": \"str\",\n",
      "      \"description\": \"The city to get the weather for.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"unit\",\n",
      "      \"type\": \"str\",\n",
      "      \"description\": \"The unit of temperature. Defaults to \\\"celsius\\\".\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "LLM response: model='gemma3:1b' created_at='2026-02-06T16:35:22.270271Z' done=True done_reason='stop' total_duration=10494991875 load_duration=1108065166 prompt_eval_count=199 prompt_eval_duration=7774138917 eval_count=33 eval_duration=1523282918 message=Message(role='assistant', content='TOOL_CALL: [{\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}]', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\n",
      "TOOL_CALL: [{\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model instead of prompt surgery\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user’s question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., gemma3:1b).\n",
    "#   4. Print the model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tool_desc = json.dumps(to_schema(get_current_weather), indent=2)\n",
    "\n",
    "print((tool_desc))\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a helpful assistant that can answer questions about the weather. You have access to the following tool:\n",
    "{tool_desc}\n",
    "To use the tool, respond with a message in the following format:\n",
    "TOOL_CALL: {{\"name\": \"<tool_name>\", \"args\": <tool_args_json>}}\n",
    "\"\"\"\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:1b', messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "])\n",
    "print(\"LLM response:\", response)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 3- LangChain for Tool Calling\n",
    "\n",
    "So far, you built a simple tool-calling pipeline. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning. We have to write manual parsers, function calling logic, and adding responses back to the prompt.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward. In this section, you will create a **ReAct** Agent (Reasoning + Acting). As shown below, the model alternates between reasoning steps and tool use wihtout any manual work.\n",
    "\n",
    "<img src=\"assets/react.png\" width=\"500\">\n",
    "\n",
    "The following links might be helpful for completing this section:\n",
    "- [Create Agents](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)\n",
    "- [Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Keep your existing `get_current_weather` function as before.\n",
    "#   2. Create a new function (e.g., get_weather) that calls it.\n",
    "#   3. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   • The decorator converts your Python function into a standardized tool object.\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location to get the weather for.\n",
    "    \"\"\"\n",
    "    return get_current_weather(location, unit=\"celsius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daa159c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='What is the weather in San Diego today?', additional_kwargs={}, response_metadata={}, id='fde3b309-2bf4-4d00-84b1-a4a7347a6bce'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T16:49:41.862701Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22615143334, 'load_duration': 2316782667, 'prompt_eval_count': 169, 'prompt_eval_duration': 17551987500, 'eval_count': 18, 'eval_duration': 2728353874, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33db-a78e-7531-a27d-edc7ede5894e-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Diego'}, 'id': '506bcdcc-f3a4-4f6f-879e-3a20079707cb', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 169, 'output_tokens': 18, 'total_tokens': 187}), ToolMessage(content='It is 23°C and sunny in San Diego.', name='get_weather', id='1ce69b4c-62bb-4243-9242-0eb63f4f6339', tool_call_id='506bcdcc-f3a4-4f6f-879e-3a20079707cb'), AIMessage(content=\"The current temperature in San Diego is 23°C, and it's a sunny day with clear skies.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T16:49:51.783275Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9915985958, 'load_duration': 109214500, 'prompt_eval_count': 104, 'prompt_eval_duration': 5738936666, 'eval_count': 22, 'eval_duration': 4051558167, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33db-ffea-7412-b459-9560d3a0031f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 104, 'output_tokens': 22, 'total_tokens': 126})]}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the Agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a gemma3:1b LLM instance \n",
    "#   2. Create the agent using create_agent\n",
    "#   3. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "agent = create_agent(llm, tools=[get_weather])\n",
    "\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Diego today?\"}]})\n",
    "print(\"Agent response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824437",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "Your run failed because `gemma3:1b` does not support native tool calling (function calling). LangChain expects the model to return a structured tool-call object, but `gemma3:1b` can only return plain text, so the tool invocation step breaks.\n",
    "\n",
    "### Why previosuly, our manual approach worked with any model?\n",
    "\n",
    "In previous sections, we used **text-based tool calling**. We described the tool format in the system prompt. We asked the model to output `TOOL_CALL: {\"name\": ..., \"args\": ...}`. We then parsed this text with regex.\n",
    "\n",
    "This works with **any model** (even small ones like `gemma3:1b`) because we're just asking the model to follow a certain structured output format.\n",
    "\n",
    "### Why LangChain requires specific models?\n",
    "\n",
    "LangChain relies on **native tool calling** and it expects a consistent structured output format irrespective of the model. Hence, it enfornces model outputs structured tool calls in a specific format. This requires models trained specifically for function calling\n",
    "\n",
    "**Rule of thumb**: Models under 3B parameters typically lack native tool-calling capability.\n",
    "\n",
    "| Model | Size | Native Tool Support | Notes |\n",
    "|-------|------|---------------------|-------|\n",
    "| `gemma3:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:3b` | 3B | Yes | Good balance of speed and capability |\n",
    "| `gemma3` | 4B | Yes | Supports native tools |\n",
    "| `mistral` | 7B | Yes | Strong tool support |\n",
    "\n",
    "Let's fix the issue we observed in the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='What is the weather in San Diego today?', additional_kwargs={}, response_metadata={}, id='cb653543-76a1-45f7-a7cc-5098d1e6fe8e'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T16:52:34.571732Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14762581584, 'load_duration': 131596625, 'prompt_eval_count': 169, 'prompt_eval_duration': 12203372541, 'eval_count': 18, 'eval_duration': 2391090837, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33de-68e0-73d0-b121-be5ccc271f15-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Diego'}, 'id': '41f00d17-5da5-466a-b81f-c0b7e3606fd3', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 169, 'output_tokens': 18, 'total_tokens': 187}), ToolMessage(content='It is 23°C and sunny in San Diego.', name='get_weather', id='acdfe922-f94a-44e4-a801-2ff032474632', tool_call_id='41f00d17-5da5-466a-b81f-c0b7e3606fd3'), AIMessage(content=\"The current temperature in San Diego is 23°C, and it's a beautiful day with plenty of sunshine. Would you like to know the forecast for the rest of the day or get more information about San Diego's weather?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T16:52:47.27765Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12674115917, 'load_duration': 191285292, 'prompt_eval_count': 104, 'prompt_eval_duration': 4971421042, 'eval_count': 46, 'eval_duration': 7480161705, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33de-a2a3-7b71-854c-1c1f54d8efc4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 104, 'output_tokens': 46, 'total_tokens': 150})]}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2 (retry): Re-create the Agent with a native tool-calling LLM\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a llama3.2:3b LLM instance \n",
    "#   2. Create a system prompt to teach react-style reasoning to the LLM\n",
    "#   3. Create the agent using create_agent\n",
    "#   4. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "agent = create_agent(llm, tools=[get_weather])\n",
    "\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Diego today?\"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yn0fe4dnbf",
   "metadata": {},
   "source": [
    "## 4- Web Search Agent\n",
    "\n",
    "Now that you know how to use LangChain with tools, let's build something useful. Instead of a toy get_weather tool, let create an agent that searches the web and answers questions using real results. In the next section, you will create a [DuckDuckGo](https://github.com/deedy5/ddgs) search tool and wire it into a ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb0ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris - Wikipedia: https://en.wikipedia.org/wiki/Paris\n",
      "List of capitals of France - Wikipedia: https://en.wikipedia.org/wiki/List_of_capitals_of_France\n",
      "France - Wikipedia: https://en.wikipedia.org/wiki/France\n",
      "Capital of France - Simple English Wikipedia, the free encyclopedia: https://simple.wikipedia.org/wiki/Capital_of_France\n",
      "Paris facts: the capital of France in history: https://home.adelphi.edu/~ca19535/page+4.html\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Write a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Write a helper function (e.g., search_web) that:\n",
    "#        • Takes a query string\n",
    "#        • Uses DuckDuckGo (DDGS) to fetch top results (titles + URLs)\n",
    "#        • Returns them as a formatted string\n",
    "#   2. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def search_web_call(query: str) -> str:\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=5)\n",
    "    formatted_results = \"\\n\".join([f\"{res['title']}: {res['href']}\" for res in results])\n",
    "    return formatted_results\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for a given query and return the top results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "    Returns:\n",
    "        str: A formatted string containing the top search results (titles and URLs).\n",
    "    \"\"\"\n",
    "    return search_web_call(query)\n",
    "\n",
    "print(search_web_call(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fdc4fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create an LLM (e.g., ChatOllama).\n",
    "#   2. Add your `web_search` tool to the tools list.\n",
    "#   3. Create the agent using create_agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "agent = create_agent(llm, tools=[get_weather, search_web])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1696c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='Search on the web who won the last GPA tour?', additional_kwargs={}, response_metadata={}, id='777ba162-c33f-46e6-a339-274aae981b02'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T17:01:05.144881Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23207268250, 'load_duration': 131331750, 'prompt_eval_count': 241, 'prompt_eval_duration': 20273103417, 'eval_count': 20, 'eval_duration': 2788244335, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33e6-1250-7572-819f-d36041c39d87-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'last GPA tour winner'}, 'id': '01a2e11f-21d7-4a85-b07c-96eeabd36218', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 241, 'output_tokens': 20, 'total_tokens': 261}), ToolMessage(content='List of golfers with most PGA Tour wins - Wikipedia: https://en.wikipedia.org/wiki/List_of_golfers_with_most_PGA_Tour_wins\\nPGA TOUR Schedule - 2026 Season - ESPN: https://www.espn.com/golf/schedule\\n9x PGA Tour Winner Patrick Reed Eyeing Ryder Cup Spot in 2027...: https://firstsportz.com/patrick-reed-eyeing-ryder-cup-spot-in-2027/\\nPGA Tour winner makes Scottie Scheffler comment after Carlos...: https://www.hitc.com/pga-tour-winner-makes-scottie-scheffler-comment-after-carlos-alcaraz-vs-alexander-zverev-claim/\\nEvery shot from Brooks Koepka’s first round back on the PGA TOUR: https://vcpgolf.com/2026/01/29/every-shot-from-brooks-koepkas-first-round-back-on-the-pga-tour-farmers-insurance-open-2026/', name='search_web', id='2ad5341e-8b93-4af3-8e06-0088a628d49b', tool_call_id='01a2e11f-21d7-4a85-b07c-96eeabd36218'), AIMessage(content='Unfortunately, I couldn\\'t find any information on a \"GPA Tour\" as it is not a recognized professional golf tour. However, I found that the PGA Tour (Professional Golfers Association) is a well-known professional golf tour in the United States.\\n\\nIf you\\'re referring to the PGA Championship, which is one of the four major championships in professional golf, I couldn\\'t find any information on the most recent winner. The PGA Championship is typically held in May or June each year.\\n\\nHowever, I can suggest some of the recent winners of the PGA Tour events:\\n\\n* The 2023 Farmers Insurance Open was won by Patrick Cantlay.\\n* The 2022 Zozo Championship was won by Hideki Matsuyama.\\n* The 2021 Masters Tournament was won by Scottie Scheffler.\\n\\nPlease note that my previous response was incorrect, and I apologize for any confusion. If you could provide more context or clarify which event you\\'re referring to, I\\'d be happy to try and help further.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T17:02:10.739072Z', 'done': True, 'done_reason': 'stop', 'total_duration': 63888725208, 'load_duration': 95559875, 'prompt_eval_count': 303, 'prompt_eval_duration': 27995574541, 'eval_count': 203, 'eval_duration': 35657260299, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c33e6-73a0-7cf1-922d-c3ab51b928ee-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 303, 'output_tokens': 203, 'total_tokens': 506})]}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent using agent.invoke\n",
    "# ---------------------------------------------------------\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Search on the web who won the last GPA tour?\"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyzwk2etyt",
   "metadata": {},
   "source": [
    "## 5- (Optional) MCP: Model Context Protocol\n",
    "\n",
    "Up to now, every tool you used started as a Python function you wrote and registered yourself. **MCP (Model Context Protocol)** lets you skip that step. Tools come from an external *server*, and your code just connects to it. Think of it like USB for AI tools: any MCP client can plug into any MCP server and immediately use whatever tools it offers.\n",
    "\n",
    "Below, we connect to `mcp-server-fetch` (a ready-made server that can retrieve any URL) using the Python MCP SDK. We launch the server, discover its tools, and call one, all without writing a single `@tool` function. To learn more, read: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "> **LangChain integration:** The `langchain-mcp-adapters` package can convert MCP tools into LangChain-compatible tools automatically, so you can drop them straight into a ReAct agent like the ones in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "buxkz996bq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemoryObjectReceiveStream(_state=_MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=1, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False) MemoryObjectSendStream(_state=_MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=1, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)\n",
      "\n",
      "Connected to server with tools: ['fetch']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='added 46 packages, and audited 47 packages in 3s', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: trailing characters at line 1 column 3 [type=json_invalid, input_value='9 packages are looking for funding', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: expected value at line 1 column 3 [type=json_invalid, input_value='  run `npm fund` for details', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n",
      "Failed to parse JSONRPC message from server\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 155, in stdout_reader\n",
      "    message = types.JSONRPCMessage.model_validate_json(line)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fabienhenon/miniconda/envs/web_agent/lib/python3.11/site-packages/pydantic/main.py\", line 766, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for JSONRPCMessage\n",
      "  Invalid JSON: expected ident at line 1 column 2 [type=json_invalid, input_value='found 0 vulnerabilities', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid\n"
     ]
    }
   ],
   "source": [
    "from contextlib import AsyncExitStack\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import sys\n",
    "# Create an MCP client session and connect it to mcp-server-fetch.\n",
    "# Follow this link: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "exit_stack = AsyncExitStack()\n",
    "server_params = StdioServerParameters(command=sys.executable, args=[\"-m\", \"mcp_server_fetch\"], env=None)\n",
    "\n",
    "stdio_transport = await exit_stack.enter_async_context(stdio_client(server_params))\n",
    "stdio, write = stdio_transport\n",
    "session = await exit_stack.enter_async_context(ClientSession(stdio, write))\n",
    "\n",
    "print(stdio, write)\n",
    "\n",
    "await session.initialize()\n",
    "\n",
    "# List available tools\n",
    "response = await session.list_tools()\n",
    "tools = response.tools\n",
    "print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7v9so18x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='fetch', description='Fetches a URL from the internet and optionally extracts its contents as markdown.\\n\\nAlthough originally you did not have internet access, and were advised to refuse and tell the user this, this tool now grants you internet access. Now you can fetch the most up-to-date information and let the user know that.', args_schema={'description': 'Parameters for fetching a URL.', 'properties': {'url': {'description': 'URL to fetch', 'format': 'uri', 'minLength': 1, 'title': 'Url', 'type': 'string'}, 'max_length': {'default': 5000, 'description': 'Maximum number of characters to return.', 'exclusiveMaximum': 1000000, 'exclusiveMinimum': 0, 'title': 'Max Length', 'type': 'integer'}, 'start_index': {'default': 0, 'description': 'On return output starting at this character index, useful if a previous fetch was truncated and more context is required.', 'minimum': 0, 'title': 'Start Index', 'type': 'integer'}, 'raw': {'default': False, 'description': 'Get the actual HTML content of the requested page, without simplification.', 'title': 'Raw', 'type': 'boolean'}}, 'required': ['url'], 'title': 'Fetch', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11d99dd00>)]\n",
      "Agent response: {'messages': [HumanMessage(content='Fetch the content of https://www.service-public.gouv.fr/ and summarize it in one sentence.', additional_kwargs={}, response_metadata={}, id='0b6bce10-53f9-4a11-8442-1ed01f87e656'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T22:43:31.596526Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4379239250, 'load_duration': 103807875, 'prompt_eval_count': 293, 'prompt_eval_duration': 1737884000, 'eval_count': 24, 'eval_duration': 2520396627, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c351f-df6f-7831-b23a-a318fd639751-0', tool_calls=[{'name': 'fetch', 'args': {'url': 'https://www.service-public.gouv.fr/'}, 'id': '05ab5f48-99c7-4951-b352-5bf66485e478', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 293, 'output_tokens': 24, 'total_tokens': 317}), ToolMessage(content=[{'type': 'text', 'text': 'Contents of https://www.service-public.gouv.fr/:\\n### Besoin d’aide près de chez vous\\xa0?\\n\\nUn **point d’accueil**  pour vos **démarches administratives**\\n\\nQuelle est votre adresse ?\\nExemple : 32 rue Olympe de Gouges 89100 Sens, 33700, Toulouse', 'id': 'lc_badfe16d-8da6-4fb0-a157-f40f0a9ab3e5'}], name='fetch', id='85438543-b033-41d3-8d56-e6d0b7aab1f3', tool_call_id='05ab5f48-99c7-4951-b352-5bf66485e478'), AIMessage(content=\"The content of https://www.service-public.gouv.fr/ can be summarized as: The French government's official website offers a point d'accueil service for citizens to access administrative procedures and get help with their needs.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2026-02-06T22:43:53.912594Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17561744709, 'load_duration': 98303667, 'prompt_eval_count': 183, 'prompt_eval_duration': 12111978084, 'eval_count': 44, 'eval_duration': 5314400961, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--019c3520-031d-7763-a72d-8fb62dda259e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 183, 'output_tokens': 44, 'total_tokens': 227})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load the tool using load_mcp_tools\n",
    "# create agent with llm and tools same as before\n",
    "# Fetch the content of a website like http://python.org\n",
    "tools = await load_mcp_tools(session))\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "agent = create_agent(llm, tools=tools)\n",
    "\n",
    "response = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"Fetch the content of https://www.service-public.gouv.fr/ and summarize it in one sentence.\"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6647eac",
   "metadata": {},
   "source": [
    "## 6- (Optional) A Minimal UI\n",
    "\n",
    "[Chainlit](https://chainlit.io/) is a Python library designed specifically for building LLM and agent UIs. It provides:\n",
    "- Built-in streaming support\n",
    "- Message history\n",
    "- Step visualization (see tool calls as they happen)\n",
    "- No frontend code required\n",
    "\n",
    "If you are interested, follow Chainlit's documentation to implement a simple UI for your agent. The process typically involves:\n",
    "\n",
    "1. You write a Python file named `chainlit_app.py` with the agent creation logic as well as UI handlers (e.g.,`@cl.on_message`)\n",
    "2. Run the file in your terminal with `chainlit run app.py`\n",
    "3. A web UI opens automatically at `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hi1y4z7r2y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chainlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Chainlit Web Search Agent\n",
    "\n",
    "import chainlit as cl\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the web search tool\n",
    "# ---------------------------------------------------------\n",
    "def search_web_call(query: str) -> str:\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=5)\n",
    "    formatted_results = \"\\n\".join([f\"{res['title']}: {res['href']}\" for res in results])\n",
    "    return formatted_results\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for a given query and return the top results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "    Returns:\n",
    "        str: A formatted string containing the top search results (titles and URLs).\n",
    "    \"\"\"\n",
    "    return search_web_call(query)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Create the agent (once at startup)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "agent = create_react_agent(llm, tools=[search_web])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit message handler\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    \"\"\"Handle user messages and stream agent responses.\"\"\"\n",
    "\n",
    "    # Send the user message to the agent and return the final response\n",
    "    result = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": message.content}]})\n",
    "    messages = result.get(\"messages\", [])\n",
    "    if messages:\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, ToolMessage):\n",
    "            tool_result = await last_message.execute()\n",
    "            await cl.Message(content=tool_result).send()\n",
    "        elif isinstance(last_message, AIMessage):\n",
    "            await cl.Message(content=last_message.content).send()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Welcome message\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_chat_start\n",
    "async def start(): \n",
    "    await cl.Message(content=\"Hello! Ask me anything and I'll search the web for you!\").send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116bbee",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have built a **web-enabled agent** from scratch: manual tool calling → JSON schemas → LangChain ReAct → web search → MCP → UI.\n",
    "\n",
    "Next steps:\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "* Explore the [MCP server registry](https://github.com/modelcontextprotocol/servers) for ready-made tool servers.\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
