{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-level concepts of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential revision, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "0. Environment setup  \n",
    "1. Inference‑time scaling  \n",
    "  1.1 Few‑shot.   \n",
    "  1.2 Zero‑shot CoT.   \n",
    "  1.3 Self‑consistency.   \n",
    "  1.4 Sequential revisions.     \n",
    "  1.5 Tree‑of‑Thought (ToT)\n",
    "2. Training reasoning models and inspecting deepseek-r1 \n",
    "3. Deep-research agent  \n",
    "4. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "```\n",
    "\n",
    "#### Option 2: uv (Fast alternative)\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create a virtual environment and install dependencies\n",
    "uv venv .venv-deep-research && source .venv-deep-research/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Now open your notebook and switch to the `deep_research` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Setup and run Ollama serve\n",
    "\n",
    "In this project we use the `llama3.2:3b`, `qwen2.5:3b-instruct` and `deepseek-r1:1.5b` models. You can try other smaller or larger reasoning LLMs such as `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "Open terminal and run ollama:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "Then open another terminal and pull required models: \n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:1.5b\n",
    "ollama pull qwen2.5:3b-instruct\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull phi4-mini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 1‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 1.1: Few-Shot CoT\n",
    "\n",
    "Few-shot prompting provides examples before asking a new question. The model learns from the pattern and applies it to new inputs.\n",
    "\n",
    "We'll explore this with two models to understand how few-shot interacts with model capabilities:\n",
    "\n",
    "1. **GPT-2** (no instruction tuning): Doesn't reason by default. We'll see if few-shot examples can elicit reasoning.\n",
    "2. **Llama 3.2** (instruction-tuned): Already reasons naturally. We'll use few-shot to control the output format.\n",
    "\n",
    "#### GPT-2: Can few-shot examples elicit reasoning?\n",
    "\n",
    "GPT-2 is a base language model that just predicts the next token. It wasn't trained to follow instructions or reason step-by-step. Let's see what happens with and without few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac082e0f9194aeeac047ddc653ad229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: openai-community/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\n",
      "\n",
      "The rectangle is made up of 32 circles, each with its own radius, the diameter of which is 64 cm. The circle's diameter is determined by the radius of the rectangle, which is the diameter of a circle.\n",
      "\n",
      "In this case, a rectangle has diameter of 64 cm, and its radius is 64 cm.\n",
      "\n",
      "The rectangle's diameter is determined by the radius of the rectangle, which is the diameter of a circle.\n",
      "\n",
      "If the radius of the rectangle is less than the radius of the rectangle, then the rectangle is rounded, which means that its radius is zero. The rectangle is then rounded to the nearest whole number.\n",
      "\n",
      "If the radius of the rectangle is less than the radius of the rectangle, then the rectangle is rounded to the nearest whole number. The rectangle is then rounded to its nearest whole number.\n",
      "\n",
      "If the radius of the rectangle is less than the radius of the rectangle, then the rectangle is rounded to the nearest whole number. The rectangle is then rounded to its nearest whole number.\n",
      "\n",
      "The square root of the rectangle is 1.\n",
      "\n",
      "The rectangle's radius is determined by the radius of the rectangle, which is the radius of a circle.\n",
      "\n",
      "In this case, a rectangle has radius of 64\n",
      "----\n",
      "Question: A rectangle has a perimeter of 60 cm. If the length is twice the width, what is the area?\n",
      "Answer: if length is twwice the width, then we can say that length = 2 * width. The perimeter of a rectangle is given by the formula P = 2 * (length + width). Substituting the values, we get 60 = 2 * (2 * width + width) => 60 = 6 * width => width = 60 / 6 => 10. Now, we can find the length: length = 2 * width => length = 2 * 10 => 20. Finally, the area of a rectangle is given by the formula A = length * width => A = 20 * 10 => A = 200 cm^2.\n",
      "\n",
      "Question:  What is the distance to the center of a circle?\n",
      "Answer:\n",
      "\n",
      "A circle would be a rectangle with a width of 6 and a radius of 10.\n",
      "\n",
      "Question:  If we have two points of a circle, does the radius of one point of the circle change?\n",
      "Answer:\n",
      "\n",
      "If we have one point of a circle, the radius of one point of the circle changes.\n",
      "\n",
      "Question:  Is it possible to find two points of a circle?\n",
      "Answer:\n",
      "\n",
      "If we have two points of a circle, we can find two points of a rectangle.\n",
      "\n",
      "Question:  Is it possible to find two points of a rectangle with a diameter of 100 mm?\n",
      "\n",
      "Answer:\n",
      "\n",
      "If we have two points of a rectangle, we can find two points of a rectangle.\n",
      "\n",
      "Question:  Is it possible to find two points of a rectangle with a diameter of 200 mm?\n",
      "\n",
      "Answer:\n",
      "\n",
      "If we have two points of a rectangle, we can find two points of a rectangle with a diameter of 200 mm.\n",
      "\n",
      "Question:  Is it possible to find two points of a rectangle with a diameter of 300 mm?\n",
      "\n",
      "Answer:\n",
      "\n",
      "If we have\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1–2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate outputs with and without fewshot prompt and compare the difference.\n",
    "\n",
    "pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=0)\n",
    "print(pipeline(question)[0]['generated_text'])\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "fewshot_prompt = \"\"\"Question: A rectangle has a perimeter of 60 cm. If the length is twice the width, what is the area?\n",
    "Answer: if length is twwice the width, then we can say that length = 2 * width. The perimeter of a rectangle is given by the formula P = 2 * (length + width). Substituting the values, we get 60 = 2 * (2 * width + width) => 60 = 6 * width => width = 60 / 6 => 10. Now, we can find the length: length = 2 * width => length = 2 * 10 => 20. Finally, the area of a rectangle is given by the formula A = length * width => A = 20 * 10 => A = 200 cm^2.\n",
    "\n",
    "Question: \"\"\"\n",
    "\n",
    "final_qquestion = fewshot_prompt + question\n",
    "\n",
    "print(pipeline(fewshot_prompt)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "#### Llama 3.2: Using few-shot to control output format\n",
    "\n",
    "Unlike GPT-2, Llama 3.2 is instruction-tuned and already produces reasoning traces by default. So what's the point of few-shot examples?\n",
    "\n",
    "**The power of few-shot with instruction-tuned models is controlling the output format.** We can make the model follow a specific structure like `[GIVEN]/[FIND]/[SOLVE]/[ANSWER]` that it wouldn't use naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the area of the rectangle with a given perimeter and a length that is twice the width, we can follow these steps:\n",
      "\n",
      "The perimeter of a rectangle is given by the formula P = 2 * (length + width). Since the length is twice the width, let's say the width is w. Then, the length is 2w.\n",
      "\n",
      "Substituting the values into the formula, we get:\n",
      "36 = 2 * (2w + w)\n",
      "36 = 6w\n",
      "Now, divide both sides by 6 to find the width:\n",
      "w = 36 / 6\n",
      "w = 6\n",
      "\n",
      "Since the length is twice the width, we can now find the length:\n",
      "length = 2 * width\n",
      "length = 2 * 6\n",
      "length = 12\n",
      "\n",
      "Finally, we can find the area of the rectangle using the formula A = length * width:\n",
      "A = 12 * 6\n",
      "A = 72 cm^2\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# Step 1: Create your Ollama client\n",
    "# Step 2: Write a few examples showing reasoning steps\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer with and without few shot examples and compare them.\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': final_qquestion,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 1.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the area of the rectangle, we need to first determine its dimensions (length and width). We know that the length is twice the width.\n",
      "\n",
      "Let's denote the width as \"w\" cm. Since the length is twice the width, the length can be represented as 2w cm.\n",
      "\n",
      "The perimeter of a rectangle is given by the formula: Perimeter = 2(length + width)\n",
      "\n",
      "We are told that the perimeter is 36 cm, so we can set up an equation:\n",
      "\n",
      "2(2w + w) = 36\n",
      "\n",
      "Combine like terms:\n",
      "4w + 2w = 36\n",
      "6w = 36\n",
      "\n",
      "Now, divide both sides by 6 to solve for \"w\":\n",
      "w = 36/6\n",
      "w = 6\n",
      "\n",
      "So, the width of the rectangle is 6 cm.\n",
      "\n",
      "Since the length is twice the width, we can now find the length:\n",
      "Length = 2w = 2(6) = 12 cm\n",
      "\n",
      "Now that we have both dimensions (length and width), we can calculate the area:\n",
      "\n",
      "Area = Length x Width\n",
      "= 12 x 6\n",
      "= 72 square centimeters\n",
      "\n",
      "Therefore, the area of the rectangle is 72 square centimeters.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# Step 1: Create your Ollama client\n",
    "# Step 2: Write a question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 3: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': question + \" Let's think step by step.\",\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 1.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the square root of 144, we can use the following steps:\n",
      "\n",
      "1. Start with a number, let's say x.\n",
      "2. Square x to get 144.\n",
      "\n",
      "Let's try x = 12:\n",
      "12^2 = 144\n",
      "\n",
      "So, x = 12 is a perfect square of 144.\n",
      "\n",
      "Final Answer: 12\n",
      "To find the square root of 144, let's break it down step by step:\n",
      "\n",
      "1. Start with the number 144.\n",
      "2. Look for a perfect square that is less than or equal to 144, such as 121 (11^2) and 169 (13^2).\n",
      "3. Since 12^2 = 144, we can conclude that the square root of 144 is 12.\n",
      "\n",
      "Final Answer: 12\n",
      "To find the square root of 144, let's break it down:\n",
      "\n",
      "1. Start with the number: 144\n",
      "2. Look for perfect squares around this number.\n",
      "3. Notice that 12 squared is equal to 144 (12² = 144).\n",
      "\n",
      "Therefore, the square root of 144 is 12.\n",
      "\n",
      "Final Answer: 12\n",
      "To find the square root of 144, we'll start by thinking about what numbers multiplied together give us 144.\n",
      "\n",
      "Let's try to think of a number that when squared (multiplied by itself) gives us 144. We can also look for perfect squares in our minds that are close to 144.\n",
      "\n",
      "A few perfect squares come to mind:\n",
      "\n",
      "* 121 is the square of 11, since 11 x 11 = 121.\n",
      "* Another option is the square of 12, since 12 x 12 = 144.\n",
      "\n",
      "Since we want a number whose square equals 144, that number must be 12 (because 12 x 12 = 144).\n",
      "\n",
      "Therefore, the square root of 144 is:\n",
      "\n",
      "Final Answer: 12\n",
      "To find the square root of 144, we can use a few different methods.\n",
      "\n",
      "One way to do this is to start with perfect squares that are close to 144. For example:\n",
      "\n",
      "- 11 x 11 = 121\n",
      "- 12 x 12 = 144\n",
      "\n",
      "Since 121 and 144 are very close together, it's likely that the square root of 144 will be somewhere between 11 and 12.\n",
      "\n",
      "However, a more precise method would be to use long division. We can divide 144 by perfect squares until we get a whole number.\n",
      "\n",
      "Here's how you would do this:\n",
      "\n",
      "√144 = ?\n",
      "\n",
      "First, try dividing 144 by 16 (since 4 x 4 = 16):\n",
      "144 ÷ 16 = 9\n",
      "\n",
      "Next, we need to check if there is any remainder left after division by 16:\n",
      "144 - (16 * 9) = 0\n",
      "144 is perfectly divisible by 16.\n",
      "\n",
      "Therefore, the square root of 144 can be expressed as:\n",
      "\n",
      "√144 = √(16 x 9)\n",
      "Since √16 = 4 and we know that √9 is 3, we have:\n",
      "√144 = 4 × 3\n",
      "\n",
      "Final Answer: 12\n",
      "Votes: Counter({'12': 5})\n",
      "Chosen answer: 12\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def cot_answer(question, temperature=1.2):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=question + \" Let's think step by step. Give the final answer in the format 'Final Answer: <your answer>'.\",\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    match = re.search(r\"Final Answer: (.*)\", resp.output_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def self_consistent(question, n=5):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    answers = [a for a in (cot_answer(question) for _ in range(n)) if a is not None]\n",
    "    counter = collections.Counter(answers)\n",
    "    winner = counter.most_common(1)[0][0] if counter else None\n",
    "    return winner, counter, answers\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter, traces = self_consistent(question, n=5)\n",
    "\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 1.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft after step 1:\n",
      "To determine how much you can pay yourself per month without jeopardizing your financial stability, we need to consider several factors.\n",
      "\n",
      "1. Annual income: €120,000\n",
      "2. Common expenses: €5,000 per year\n",
      "3. Monthly project development budget: €3,000 per month\n",
      "4. Taxes: 35% of annual income\n",
      "\n",
      "First, let's calculate the total amount available for personal use:\n",
      "\n",
      "Annual income: €120,000\n",
      "Taxes (35%): €42,000 (calculated as 0.35 x €120,000)\n",
      "Remaining income after taxes: €78,000\n",
      "\n",
      "Common expenses per year: €5,000\n",
      "Monthly common expenses: €416.67 (calculated as €5,000 / 12)\n",
      "\n",
      "Monthly project development budget: €3,000\n",
      "\n",
      "Total monthly deductions:\n",
      "€416.67 (common expenses) + €3,000 (project development) = €4,416.67\n",
      "\n",
      "Now, let's calculate how much you can afford to pay yourself per month:\n",
      "\n",
      "Remaining income after taxes: €78,000\n",
      "Total monthly deductions: €4,416.67\n",
      "Available for personal use: €73,583.33 (calculated as €78,000 - €4,416.67)\n",
      "\n",
      "To ensure financial stability, it's recommended that you allocate no more than 50% of your available income towards your personal salary. Based on this, let's calculate a safe monthly salary range:\n",
      "\n",
      "Available for personal use: €73,583.33\n",
      "Safe percentage allocation (50%): €36,791.67 (calculated as 0.5 x €73,583.33)\n",
      "\n",
      "This means you can safely pay yourself between €36,791.67 and approximately €40,000 per month to ensure financial stability.\n",
      "\n",
      "Keep in mind that this is just a rough estimate, and you should adjust the calculation based on your individual circumstances, such as:\n",
      "\n",
      "* Other financial obligations (loans, debts, etc.)\n",
      "* Savings goals\n",
      "* Emergency fund requirements\n",
      "* Specific business needs\n",
      "\n",
      "It's always better to err on the side of caution and allocate a smaller amount towards personal expenses.\n",
      "--------------------------------------------------\n",
      "Draft after step 2:\n",
      "To determine how much you can pay yourself per month without jeopardizing your financial stability, we need to consider several factors.\n",
      "\n",
      "1. Annual income: €120,000\n",
      "2. Common expenses: €5,000 per year\n",
      "3. Monthly project development budget: €3,000 per month\n",
      "4. Taxes: 35% of annual income\n",
      "\n",
      "First, let's calculate the total amount available for personal use:\n",
      "\n",
      "Annual income: €120,000\n",
      "Taxes (35%): €42,000 (calculated as 0.35 x €120,000)\n",
      "Remaining income after taxes: €78,000 per year\n",
      "\n",
      "Common expenses per year: €5,000\n",
      "Monthly common expenses: €416.67 (calculated as €5,000 / 12)\n",
      "\n",
      "To calculate the total monthly deductions:\n",
      "- Common expenses: €416.67\n",
      "- Project development budget: €3,000\n",
      "Total monthly deductions for fixed expenses: €4,416.67\n",
      "\n",
      "Next, let's calculate your disposable income per month:\n",
      "\n",
      "Remaining income after taxes: €78,000\n",
      "Total monthly deductions for fixed expenses: €4,416.67\n",
      "Disposable income per year (excluding project funding): €73,583.33 (calculated as €78,000 - €4,416.67)\n",
      " Disposable income per month: €6,130.56 (calculated as €73,583.33 / 12)\n",
      "\n",
      "Since the goal is to determine how much you can afford to pay yourself while still maintaining financial stability, we will apply a rule of thumb for salaries.\n",
      "\n",
      "Consider using the \"50/30/20 Rule\": Allocate no more than 50% of your income towards necessary expenses (housing, utilities, food), 30% towards discretionary spending (entertainment, hobbies, personal interests), and 20% towards saving and debt repayment. Keep in mind that this is just a rough guideline, but it provides a foundation for creating a sustainable budget.\n",
      "\n",
      "Given the need to allocate €3,000 each month for project development, your salary would have a direct impact on how much you can save or spend on other things.\n",
      "\n",
      "Now let's find out how much you can pay yourself per month using a more cautious approach:\n",
      " \n",
      " Disposable income available:  €6,130.56\n",
      " Fixed monthly deductions:  €4,416.67\n",
      " Disposable income left over (€ 6,130.56- €4,416.67= ) : € 713.89 \n",
      " \n",
      "Assuming the common expenses have already been factored into this amount to avoid overspending we are looking for your take home pay which should be slightly less, here's how it can look.\n",
      "\n",
      "If you want to save at least 20% and allow for some buffer against emergencies while following this \"50/30/20 Rule\" you may consider keeping your personal salary around 80-90% of your net disposable income:\n",
      " \n",
      "€713.89*0.8 = €570.73\n",
      " €713.89*0.9= €641.16\n",
      " \n",
      "These figures represent a more conservative approach, suggesting that a monthly salary between €570 and €650 could be suitable while maintaining some buffer for unexpected expenses.\n",
      "\n",
      "Please note that this is just an estimate, and it's always a good idea to adjust the calculation based on your individual circumstances:\n",
      "\n",
      "* Other financial obligations (loans, debts, etc.)\n",
      "* Savings goals\n",
      "* Emergency fund requirements\n",
      "* Specific business needs\n",
      "\n",
      "Keep in mind that it's better to err on the side of caution when it comes to personal finances.\n",
      "--------------------------------------------------\n",
      "To determine how much you can pay yourself per month without jeopardizing your financial stability, we need to consider several factors.\n",
      "\n",
      "1. Annual income: €120,000\n",
      "2. Common expenses: €5,000 per year\n",
      "3. Monthly project development budget: €3,000 per month\n",
      "4. Taxes: 35% of annual income\n",
      "\n",
      "First, let's calculate the total amount available for personal use:\n",
      "\n",
      "Annual income: €120,000\n",
      "Taxes (35%): €42,000 (calculated as 0.35 x €120,000)\n",
      "Remaining income after taxes: €78,000 per year\n",
      "\n",
      "Common expenses per year: €5,000\n",
      "Monthly common expenses: €416.67 (calculated as €5,000 / 12)\n",
      "\n",
      "To calculate the total monthly deductions:\n",
      "- Common expenses: €416.67\n",
      "- Project development budget: €3,000\n",
      "Total monthly deductions for fixed expenses: €4,416.67\n",
      "\n",
      "Next, let's calculate your disposable income per month:\n",
      "\n",
      "Remaining income after taxes: €78,000\n",
      "Total monthly deductions for fixed expenses: €4,416.67\n",
      "Disposable income per year (excluding project funding): €73,583.33 (calculated as €78,000 - €4,416.67)\n",
      " Disposable income per month: €6,130.56 (calculated as €73,583.33 / 12)\n",
      "\n",
      "To determine a suitable monthly salary, we can consider the following:\n",
      "\n",
      "1. The \"50/30/20 Rule\": Allocate no more than 50% of your income towards necessary expenses (housing, utilities, food), 30% towards discretionary spending (entertainment, hobbies, personal interests), and 20% towards saving and debt repayment.\n",
      "2. A buffer for unexpected expenses: Aim to keep 10-20% of your net disposable income as an emergency fund.\n",
      "\n",
      "Considering these factors, let's calculate a suitable monthly salary range:\n",
      "\n",
      "Disposable income per month: €6,130.56\n",
      "Recommended buffer for emergencies: 15% (calculated as 0.15 x €6,130.56)\n",
      "Recommended monthly savings: €918.69 (calculated as €6,130.56 x 0.15)\n",
      "\n",
      "Now, let's subtract the total monthly deductions from the disposable income to determine a suitable monthly salary:\n",
      "\n",
      "Disposable income per month: €6,130.56\n",
      "Total monthly deductions for fixed expenses: €4,416.67\n",
      "Recommended monthly take-home pay: €1,713.89 (calculated as €6,130.56 - €4,416.67)\n",
      "\n",
      "To be on the safe side, considering other factors such as business needs and personal goals, you may want to aim for a lower salary range:\n",
      "\n",
      "Recommended monthly salary range: 80-90% of net disposable income\n",
      " Recommended monthly salary: €1,317.47 (calculated as €6,130.56 x 0.8) to €1,455.18 (calculated as €6,130.56 x 0.9)\n",
      "\n",
      "Please note that these figures are estimates and may vary based on your individual circumstances:\n",
      "\n",
      "* Other financial obligations (loans, debts, etc.)\n",
      "* Savings goals\n",
      "* Emergency fund requirements\n",
      "* Specific business needs\n",
      "\n",
      "It's always better to err on the side of caution when it comes to personal finances.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    input = [{\"role\": \"user\", \"content\": question + \" Produce a first draft of the answer.\"}]\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=input,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    # Add output to the input for the next step\n",
    "    draft = resp.output_text\n",
    "    input.append({\"role\": \"assistant\", \"content\": draft})\n",
    "\n",
    "    for step in range(1, max_steps):\n",
    "        print(f\"Draft after step {step}:\\n{draft}\\n{'-'*50}\")\n",
    "        input.append({\"role\": \"user\", \"content\": \"Please revise and improve your answer.\"})\n",
    "\n",
    "        resp = client.responses.create(\n",
    "            model=MODEL,\n",
    "            input=input,\n",
    "            temperature=1.2,\n",
    "        )\n",
    "        draft = resp.output_text\n",
    "        input.append({\"role\": \"assistant\", \"content\": draft})\n",
    "\n",
    "    return draft\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "# Step 3: Print the final output\n",
    "question = \"The income of my company is roughly 120,000€ per year. I need 5000€ per year for common expenses. I need 3000€ per month to develop another project. I have to pay 35% of my salary in taxes. How much money can I pay myself per month as a salary without jeopardizing my financial stability?\"\n",
    "final_answer = sequential_revision(question, max_steps=3)\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 1.5 Tree-of-Thoughts\n",
    "\n",
    "Tree-of-Thoughts (ToT) reframes reasoning as a search problem. Instead of generating one linear chain of thoughts, the model:\n",
    "1. Generates multiple candidate \"thoughts\" at each step\n",
    "2. Evaluates how promising each thought is\n",
    "3. Expands only the best candidates (beam search)\n",
    "4. Backtracks if needed\n",
    "\n",
    "This mirrors how humans solve hard problems: brainstorm options, evaluate them, pursue the best, and backtrack when stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpugbki39kv",
   "metadata": {},
   "source": [
    "#### Example 1: Word Ladder (Algorithmic ToT)\n",
    "\n",
    "This example shows ToT as pure beam search without LLM calls. Each \"thought\" is a candidate word that differs by one letter. We score by edit distance to goal and keep the best candidates.\n",
    "\n",
    "This demonstrates the **core algorithm** behind ToT: expand, score, prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    neighbors = []\n",
    "    for i in range(len(word)):\n",
    "        for c in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            mutated = word[:i] + c + word[i+1:]\n",
    "            if mutated in vocabulary and mutated != word:\n",
    "                neighbors.append(mutated)\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    frontier = [[start]]\n",
    "    for depth in range(max_depth):\n",
    "        new_frontier = []\n",
    "        for path in frontier:\n",
    "            last_word = path[-1]\n",
    "            for neighbor in neighbors(last_word, vocab):\n",
    "                new_path = path + [neighbor]\n",
    "                new_frontier.append(new_path)\n",
    "        # Score paths by edit distance to goal\n",
    "        scored_paths = [(path, 1 - SequenceMatcher(None, path[-1], goal).ratio()) for path in new_frontier]\n",
    "        scored_paths.sort(key=lambda x: x[1])  # Sort by score (edit distance)\n",
    "        frontier = [path for path, score in scored_paths[:beam_width]]  # Keep top beam_width paths\n",
    "        if any(path[-1] == goal for path in frontier):\n",
    "            return next(path for path in frontier if path[-1] == goal)  # Return the first path that reaches the goal\n",
    "    return None  # No path found within max_depth\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k84w1nz2u6p",
   "metadata": {},
   "source": [
    "#### Example 2: Generic ToT for Open-Ended Problems\n",
    "\n",
    "For open-ended problems without verifiable answers, we can still apply ToT by having the LLM both propose and evaluate thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution (score 6):\n",
      "**Weekend Science Workshop Plan for 12-Year-Olds**\n",
      "**Objective:** To provide an engaging and interactive science experience for 12-year-olds, promoting hands-on learning, critical thinking, and curiosity about the natural world.\n"
     ]
    }
   ],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next “thoughts” that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client. Then return a list of stripped strings.\n",
    "    prompt = f\"Question: {question}\\nCurrent state: {state}\\nPropose up to {k} next thoughts to extend this state:\"\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=prompt,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    thoughts = [line.strip() for line in resp.output_text.splitlines() if line.strip()]\n",
    "    return thoughts[:k]  # Return up to k thoughts\n",
    "\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1–10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1–10;\n",
    "    prompt = f\"Question: {question}\\nCurrent state: {state}\\nRate the promise of this state on a scale of 1-10:\"\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=prompt,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    match = re.search(r'\\b([1-9]|10)\\b', resp.output_text)\n",
    "    return int(match.group(0)) if match else 0\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    frontier = [(\"\", 0)]  # List of (state, score)\n",
    "    for d in range(depth):\n",
    "        new_frontier = []\n",
    "        for state, _ in frontier:\n",
    "            thoughts = propose_thoughts(question, state, k=width)\n",
    "            for thought in thoughts:\n",
    "                new_state = state + \"\\n\" + thought if state else thought\n",
    "                score = score_state(question, new_state)\n",
    "                new_frontier.append((new_state, score))\n",
    "        # Sort by score desc and keep top 'width'\n",
    "        new_frontier.sort(key=lambda x: x[1], reverse=True)\n",
    "        frontier = new_frontier[:width]\n",
    "    best_state, best_score = frontier[0]\n",
    "    return best_state, best_score\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Training Models for Reasoning\n",
    "\n",
    "### 2.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 2.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e7757",
   "metadata": {},
   "source": [
    "### 2.3 Inspect a reasoning model\n",
    "\n",
    "Now that we've discussed how reasoning models are trained, let's see one in action. We'll use **DeepSeek-R1**, a reasoning model that produces explicit *thinking tokens* before giving its final answer. The model wraps its internal chain-of-thought inside `<think>...</think>` tags, followed by a clean final response.\n",
    "\n",
    "In the cell below we send a question to DeepSeek-R1 and parse the output to separate:\n",
    "- **Thinking tokens** — the model's internal reasoning process (hidden from the end user in production).\n",
    "- **Final answer** — the polished response the user actually sees.\n",
    "\n",
    "We use `deepseek-r1:1.5b` here for speed. You can switch to `deepseek-r1:8b` for higher-quality reasoning, but it will take longer to run. Pull whichever variant you want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe0db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_622599', created_at=1771178050.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='deepseek-r1:1.5b', object='response', output=[ResponseReasoningItem(id='rs_resp_622599', summary=[Summary(text='Okay, so I need to find the area of a rectangle where the perimeter is 36 cm and the length is twice the width. Hmm, let me think about how to approach this step by step.\\n\\nFirst off, rectangles have opposite sides that are equal in length. So if one pair of sides is \\'length\\' (which we\\'ll call L) and the other is \\'width\\' (which we can call W), then there are two sides each with length L and width W.\\n\\nThe formula for the perimeter of a rectangle is:  \\nPerimeter = 2*(Length + Width).  \\n\\nThey told us that the perimeter is 36 cm. So plugging into the formula:\\n\\n2*(L + W) = 36 cm.\\n\\nAdditionally, they said the length is twice the width. That means L = 2*W. Okay, so I can substitute this into our equation for the perimeter to solve for the width first, and then find the length.\\n\\nSo substituting L with 2W in the perimeter equation:\\n\\n2*(2W + W) = 36.\\n\\nSimplify inside the parentheses: 2W + W is 3W. So now it\\'s:\\n\\n2*3W = 36,\\nWhich is:\\n6W = 36 cm.\\n\\nTo find W, divide both sides by 6:\\n\\nW = 36/6 = 6 cm.\\n\\nSo the width is 6 cm. Now since length is twice the width, L = 2*W = 2*6 cm = 12 cm.\\n\\nNow that I have both dimensions, length and width, calculating the area should be straightforward. The formula for the area of a rectangle is:\\n\\nArea = Length * Width.\\n\\nSo plugging in our values:\\n\\nA = 12 cm * 6 cm = 72 cm².\\n\\nWait, hold on a second. Let me double-check my steps because that seems too quick. Sometimes when I work through problems step by step, especially under time pressure, I might make an error.\\n\\nStarting again: Perimeter is 36 cm. So formula is 2*(L + W) = 36, so L + W = 18. Given L = 2W, substitute into that equation:\\n\\n2W + W = 18 => 3W = 18 => W = 6.\\n\\nThat still gives me width as 6 and length as 12. So area is indeed 12*6=72 cm². Wait, 72 cm squared... That does seem correct.\\n\\nBut just to make sure I\\'m not missing something: sometimes with word problems, it\\'s easy to mix up which side is length or width. But since they specifically asked for the area without specifying units other than centimeters, I think 72 cm² should be fine.\\n\\nAlternatively, if someone made a mistake and thought that the perimeter would be 36 in different terms, but nope, perimeters are straightforward just with L + W multiplied by two. So unless there\\'s some trick here... Hmm, it says \"a rectangle has a perimeter of 36 cm,\" so no tricks or something else.\\n\\nWait another thought: sometimes people get confused between area and perimeter when similar words are used in problems. Just to make sure my area is correct:\\n\\nIf L=12, W=6, then area = length*width =12*6 which is indeed 72. Yeah, that\\'s consistent.\\n\\nAlternatively, if I didn\\'t use the perimeter equation, would there be another way? Hmm. Maybe set up the equations differently... No, actually using linear systems makes sense here since we have two variables: L and W. And given that they are related by a multiple (length is twice width), substitution seems straightforward.\\n\\nAnother double-checking: if length is 12 cm, perimeter would be 2*(12+6)=2*18=36 cm, which matches the problem\\'s statement. Similarly, area being 72 cm² also aligns with those numbers.\\n\\nTherefore, I think that my answer is correct, Area = 72 cm².\\n\\n**Final Answer**\\nThe area of the rectangle is \\\\boxed{72} square centimeters.\\n', type='summary_text')], type='reasoning', content=None, encrypted_content='Okay, so I need to find the area of a rectangle where the perimeter is 36 cm and the length is twice the width. Hmm, let me think about how to approach this step by step.\\n\\nFirst off, rectangles have opposite sides that are equal in length. So if one pair of sides is \\'length\\' (which we\\'ll call L) and the other is \\'width\\' (which we can call W), then there are two sides each with length L and width W.\\n\\nThe formula for the perimeter of a rectangle is:  \\nPerimeter = 2*(Length + Width).  \\n\\nThey told us that the perimeter is 36 cm. So plugging into the formula:\\n\\n2*(L + W) = 36 cm.\\n\\nAdditionally, they said the length is twice the width. That means L = 2*W. Okay, so I can substitute this into our equation for the perimeter to solve for the width first, and then find the length.\\n\\nSo substituting L with 2W in the perimeter equation:\\n\\n2*(2W + W) = 36.\\n\\nSimplify inside the parentheses: 2W + W is 3W. So now it\\'s:\\n\\n2*3W = 36,\\nWhich is:\\n6W = 36 cm.\\n\\nTo find W, divide both sides by 6:\\n\\nW = 36/6 = 6 cm.\\n\\nSo the width is 6 cm. Now since length is twice the width, L = 2*W = 2*6 cm = 12 cm.\\n\\nNow that I have both dimensions, length and width, calculating the area should be straightforward. The formula for the area of a rectangle is:\\n\\nArea = Length * Width.\\n\\nSo plugging in our values:\\n\\nA = 12 cm * 6 cm = 72 cm².\\n\\nWait, hold on a second. Let me double-check my steps because that seems too quick. Sometimes when I work through problems step by step, especially under time pressure, I might make an error.\\n\\nStarting again: Perimeter is 36 cm. So formula is 2*(L + W) = 36, so L + W = 18. Given L = 2W, substitute into that equation:\\n\\n2W + W = 18 => 3W = 18 => W = 6.\\n\\nThat still gives me width as 6 and length as 12. So area is indeed 12*6=72 cm². Wait, 72 cm squared... That does seem correct.\\n\\nBut just to make sure I\\'m not missing something: sometimes with word problems, it\\'s easy to mix up which side is length or width. But since they specifically asked for the area without specifying units other than centimeters, I think 72 cm² should be fine.\\n\\nAlternatively, if someone made a mistake and thought that the perimeter would be 36 in different terms, but nope, perimeters are straightforward just with L + W multiplied by two. So unless there\\'s some trick here... Hmm, it says \"a rectangle has a perimeter of 36 cm,\" so no tricks or something else.\\n\\nWait another thought: sometimes people get confused between area and perimeter when similar words are used in problems. Just to make sure my area is correct:\\n\\nIf L=12, W=6, then area = length*width =12*6 which is indeed 72. Yeah, that\\'s consistent.\\n\\nAlternatively, if I didn\\'t use the perimeter equation, would there be another way? Hmm. Maybe set up the equations differently... No, actually using linear systems makes sense here since we have two variables: L and W. And given that they are related by a multiple (length is twice width), substitution seems straightforward.\\n\\nAnother double-checking: if length is 12 cm, perimeter would be 2*(12+6)=2*18=36 cm, which matches the problem\\'s statement. Similarly, area being 72 cm² also aligns with those numbers.\\n\\nTherefore, I think that my answer is correct, Area = 72 cm².\\n\\n**Final Answer**\\nThe area of the rectangle is \\\\boxed{72} square centimeters.\\n', status=None), ResponseOutputMessage(id='msg_428784', content=[ResponseOutputText(annotations=[], text='Given a rectangle with a perimeter of 36 cm and the length being twice the width, we need to find the area.\\n\\nFirst, recall the formula for the perimeter of a rectangle:\\n\\\\[ \\\\text{Perimeter} = 2 \\\\times (\\\\text{Length} + \\\\text{Width}) \\\\]\\n\\nLet \\\\( L \\\\) be the length and \\\\( W \\\\) be the width. We know that \\\\( L = 2W \\\\). Substituting this into the perimeter equation gives:\\n\\\\[ 2 \\\\times (2W + W) = 36 \\\\]\\nSimplifying inside the parentheses:\\n\\\\[ 2 \\\\times 3W = 36 \\\\]\\n\\\\[ 6W = 36 \\\\]\\n\\nSolving for \\\\( W \\\\):\\n\\\\[ W = \\\\frac{36}{6} = 6 \\\\text{ cm} \\\\]\\n\\nSince \\\\( L = 2W \\\\), we find the length:\\n\\\\[ L = 2 \\\\times 6 = 12 \\\\text{ cm} \\\\]\\n\\nThe area of a rectangle is given by:\\n\\\\[ \\\\text{Area} = \\\\text{Length} \\\\times \\\\text{Width} \\\\]\\nSubstituting the values:\\n\\\\[ \\\\text{Area} = 12 \\\\times 6 = 72 \\\\text{ cm}^2 \\\\]\\n\\nThus, the area of the rectangle is \\\\boxed{72} square centimeters.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.2, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1771178050.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=27, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=1173, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=1200), user=None, presence_penalty=0, frequency_penalty=0, store=False)\n",
      "Thinking:\n",
      " No thinking found\n",
      "Final Answer:\n",
      " Given a rectangle with a perimeter of 36 cm and the length being twice the width, we need to find the area.\n",
      "\n",
      "First, recall the formula for the perimeter of a rectangle:\n",
      "\\[ \\text{Perimeter} = 2 \\times (\\text{Length} + \\text{Width}) \\]\n",
      "\n",
      "Let \\( L \\) be the length and \\( W \\) be the width. We know that \\( L = 2W \\). Substituting this into the perimeter equation gives:\n",
      "\\[ 2 \\times (2W + W) = 36 \\]\n",
      "Simplifying inside the parentheses:\n",
      "\\[ 2 \\times 3W = 36 \\]\n",
      "\\[ 6W = 36 \\]\n",
      "\n",
      "Solving for \\( W \\):\n",
      "\\[ W = \\frac{36}{6} = 6 \\text{ cm} \\]\n",
      "\n",
      "Since \\( L = 2W \\), we find the length:\n",
      "\\[ L = 2 \\times 6 = 12 \\text{ cm} \\]\n",
      "\n",
      "The area of a rectangle is given by:\n",
      "\\[ \\text{Area} = \\text{Length} \\times \\text{Width} \\]\n",
      "Substituting the values:\n",
      "\\[ \\text{Area} = 12 \\times 6 = 72 \\text{ cm}^2 \\]\n",
      "\n",
      "Thus, the area of the rectangle is \\boxed{72} square centimeters.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Create OpenAI client and set your DeepSeek Model\n",
    "# Step 2: Write a math question\n",
    "# Step 3: Call your model\n",
    "# Step 4: Inspect the output. Separate thinking and final answer sections and print them.\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"deepseek-r1:1.5b\"\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=question,\n",
    "    temperature=1.2,\n",
    "    reasoning={\"effort\": \"medium\"}\n",
    ")\n",
    "\n",
    "# Parse thinkning and final answer. Thinking part is between <think> and </think> tags. Final answer is the rest.\n",
    "thinking_match = re.search(r\"<think>(.*?)</think>\", resp.output_text, re.DOTALL)\n",
    "thinking = thinking_match.group(1).strip() if thinking_match else \"No thinking found\"\n",
    "final_answer = re.sub(r\"<think>.*?</think>\", \"\", resp.output_text, flags=re.DOTALL).strip()\n",
    "\n",
    "print(resp)\n",
    "\n",
    "print(\"Thinking:\\n\", thinking)\n",
    "print(\"Final Answer:\\n\", final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with an LLM in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasons and decides to use tools\n",
    "2. The agent searches and feeds condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `create_agent` from `langchain.agents`, which builds a ReAct-style agent graph. Note: the agent model must support **tool calling** (e.g., `llama3.2:3b`). Models like `deepseek-r1` are reasoning models that do not support native tool calling and cannot be used directly as the agent LLM. We can stick to the `llama3.2:3b` or `qwen2.5:3b-instruct` for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    \"\"\"\"\n",
    "    Run a simple web search and return joined snippets.\n",
    "    \"\"\"\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=k)\n",
    "    snippets = [result['body'] for result in results]\n",
    "    return \"\\n\".join(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impersonate 'safari_15.3' does not exist, using 'random'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What are the best resources to learn machine learning in 2026?', additional_kwargs={}, response_metadata={}, id='ffe1a356-0d39-4940-b822-ecec78d26bd9'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b-instruct', 'created_at': '2026-02-15T17:58:48.062176Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3278058458, 'load_duration': 83512458, 'prompt_eval_count': 172, 'prompt_eval_duration': 106910916, 'eval_count': 31, 'eval_duration': 3062504791, 'logprobs': None, 'model_name': 'qwen2.5:3b-instruct', 'model_provider': 'ollama'}, id='lc_run--019c6274-732e-7433-9f0a-14f6f8208a6c-0', tool_calls=[{'name': 'ddg_search', 'args': {'query': 'best resources to learn machine learning 2026'}, 'id': '70e917df-d29d-42fa-8be6-f15de77f490f', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 172, 'output_tokens': 31, 'total_tokens': 203}), ToolMessage(content='\\n\" Professional Certificate in AI and MachineLearning by Simplilearn in collaboration with Purdue University - https://www.simplilearn.com/pgp-ai-machine-...\\nColab is especially well suited to machinelearning, data science, and education.Check out these resourcestolearn more about Colab and its ever-expanding ecosystem.\\nMachineLearning Algorithms | Introduction and Applications. How To Choose MachineLearning Models.MachineLearning Roadmap 2026.\\nIs machinelearning still worth learning in 2026? Learn when ML matters, when AI tools are enough, and how to future-proof your AI career.', name='ddg_search', id='f1bd8659-de9a-43b8-b85a-419e00049ae7', tool_call_id='70e917df-d29d-42fa-8be6-f15de77f490f'), AIMessage(content=\"Based on the information provided from my web search, here are some resources for learning about machine learning that could be relevant:\\n\\n1. Professional Certificate in AI and Machine Learning by Simplilearn in collaboration with Purdue University: [Visit Simplilearn](https://www.simplilearn.com/pgp-ai-machine-learning-purdue-edx)\\n\\n2. Colab (Cloud IDE): While primarily a platform for Python development, it is especially well-suited to machine learning, data science, and education. You can check out more resources on how to use Colab: [Colab Resources](https://colab.research.google.com/docs)\\n\\n3. A few articles that provide insights into the state of machine learning in 2026:\\n   - Introduction to Machine Learning Algorithms: https://www.exampleresource1.com/algorithms\\n   - Applications and Introductions on Choosing ML Models: [Resource Link](https://www.exampleresource2.com/choosing)\\n   - A general roadmap for machine learning development: [Roadmap Resource](https://www.exampleresource3.com/machine-learning-roadmap)\\n\\n4. It's also worth considering whether machine learning is still a worthwhile field to learn in 2026, and understanding when ML is necessary versus other AI tools. Here’s a potential resource: [Future of Machine Learning Resources](https://www.futuremlresources.com/).\\n\\nFor more specific or detailed resources, please provide additional information about your interests or level of expertise.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b-instruct', 'created_at': '2026-02-15T17:59:37.508747Z', 'done': True, 'done_reason': 'stop', 'total_duration': 47899026875, 'load_duration': 96123667, 'prompt_eval_count': 343, 'prompt_eval_duration': 13580286917, 'eval_count': 300, 'eval_duration': 33980680213, 'logprobs': None, 'model_name': 'qwen2.5:3b-instruct', 'model_provider': 'ollama'}, id='lc_run--019c6274-8608-7520-89cb-7eef57fe1c5d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 343, 'output_tokens': 300, 'total_tokens': 643})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL = \"qwen2.5:3b-instruct\"\n",
    "question = \"What are the best resources to learn machine learning in 2026?\"\n",
    "\n",
    "# Step 1: Initialize the LLM via ChatOllama (must support tool calling)\n",
    "client = ChatOllama(model=MODEL)\n",
    "\n",
    "# Step 2: Build a tool-calling agent with DuckDuckGo search\n",
    "agent = create_agent(\n",
    "    model=client,\n",
    "    tools=[ddg_search],\n",
    ")\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# 4- (Optional) Multi-Agent Deep Research\n",
    "\n",
    "Instead of a single agent, we can design multiple collaborating agents that work in parallel:\n",
    "\n",
    "1. **Planner**: Analyzes the query and breaks it into sub-questions\n",
    "2. **Researchers**: Run in parallel, each searching and summarizing findings for one sub-question  \n",
    "3. **Synthesizer**: Combines all research into a coherent final report\n",
    "\n",
    "This setup improves coverage and speed by parallelizing the research phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-questions:\n",
      "1. Here are 4 focused sub-questions that decompose the original research query:\n",
      "2. * What are the most popular online courses or tutorials for learning machine learning in 2026?\n",
      "3. * Which books or textbooks provide comprehensive and up-to-date information on machine learning in 2026?\n",
      "4. * Are there any specialized bootcamps, workshops, or conferences that offer hands-on experience with machine learning in 2026?\n",
      "5. * What are the most effective ways to learn machine learning through real-world projects and case studies in 2026?\n",
      "============================================================\n",
      "FINAL REPORT\n",
      "============================================================\n",
      "**Learning Machine Learning in 2026: A Comprehensive Report**\n",
      "\n",
      "**Introduction**\n",
      "As the field of machine learning continues to evolve, it is essential to identify effective resources that can help individuals develop this critical skillset. This report aims to provide an overview of the best resources available for learning machine learning in 2026.\n",
      "\n",
      "**Online Courses and Tutorials**\n",
      "While specific online courses or tutorials are not mentioned in the research findings, we can recommend some popular platforms and courses that offer comprehensive machine learning training:\n",
      "\n",
      "* [Coursera](https://www.coursera.org/specializations/machine-learning): Offers a wide range of machine learning courses from top universities.\n",
      "* [edX](https://www.edx.org/learn/mastering-machine-learning): Provides an extensive collection of machine learning courses and certifications.\n",
      "\n",
      "**Books and Textbooks**\n",
      "Unfortunately, the research findings do not provide any information on books or textbooks that offer comprehensive and up-to-date information on machine learning in 2026. However, some popular resources include:\n",
      "\n",
      "* [Pattern Recognition and Machine Learning](https://people.mie.unimelb.edu.au/~robert/grml.html): A classic textbook on machine learning.\n",
      "* [Deep Learning](https://deeplearning.mitviet.org/): A comprehensive online book and course series by Andrew Ng.\n",
      "\n",
      "**Specialized Bootcamps, Workshops, and Conferences**\n",
      "Several specialized bootcamps, workshops, and conferences offer hands-on experience with machine learning in 2026:\n",
      "\n",
      "1. **ICML 2026**: The 43rd International Conference on Machine Learning will take place from July 6-11, 2026, in Seoul, South Korea, offering tutorials and workshops for hands-on experience.\n",
      "2. **CIKM 2024**: While not exclusively focused on machine learning, the title mentions machine-learning technologies, making it a relevant resource.\n",
      "\n",
      "**Real-World Projects and Case Studies**\n",
      "To learn machine learning effectively through real-world projects and case studies in 2026, consider the following approaches:\n",
      "\n",
      "1. **Interactive Learning Environments**: Engage with interactive platforms that allow you to read lessons, take notes, and complete browser-based challenges that simulate real-world projects.\n",
      "2. **Practical, Project-Driven Learning Experiences**: Focus on project-driven learning experiences that incorporate best practices from real-world examples.\n",
      "3. **Higher Education or Certifications**: Pursue higher education or certifications from reputable institutions, such as Mohamed bin Zayed University of Artificial Intelligence, which offer market-relevant AI degrees with a global focus.\n",
      "\n",
      "By leveraging these resources and approaches, individuals can develop a solid foundation in machine learning and prepare themselves for the skills of the future.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI\n",
    "from ddgs import DDGS\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def plan_research(query: str) -> list[str]:\n",
    "    \"\"\"Planner agent: breaks query into sub-questions.\"\"\"\n",
    "    # Prompt the LLM to decompose the query into 1-5 focused sub-questions.\n",
    "    # The prompt should instruct the model to return only sub-questions, one per line.\n",
    "    # Parse the response into a list of stripped strings and return\n",
    "    prompt = f\"Decompose the following research query into 1-5 focused sub-questions, one per line:\\n\\n{query}\"\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=prompt,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    sub_questions = [line.strip() for line in resp.output_text.splitlines() if line.strip()]\n",
    "    return sub_questions\n",
    "\n",
    "\n",
    "def search_and_summarize(sub_question: str) -> dict:\n",
    "    \"\"\"Researcher agent: searches web and summarizes findings for one sub-question.\"\"\"\n",
    "    # Step 1: Use DDGS to search the web for the sub-question (max_results=3)\n",
    "    # Step 2: Join the result snippets into a single string\n",
    "    # Step 3: Prompt the LLM to write a concise summary based on the snippets\n",
    "    # Step 4: Return a dict with keys \"question\" and \"summary\"\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(sub_question, max_results=3)\n",
    "    snippets = [result['body'] for result in results]\n",
    "    joined_snippets = \"\\n\".join(snippets)\n",
    "    summary_prompt = f\"Summarize the following information in a concise way to answer the question: {sub_question}\\n\\nInformation:\\n{joined_snippets}\"\n",
    "    summary_resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=summary_prompt,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    summary = summary_resp.output_text.strip()\n",
    "    return {\"question\": sub_question, \"summary\": summary}\n",
    "\n",
    "\n",
    "def synthesize_report(query: str, findings: list[dict]) -> str:\n",
    "    \"\"\"Synthesizer agent: combines all findings into a coherent report.\"\"\"\n",
    "    # Step 1: Format the findings list into a readable text block (e.g., \"### sub-question\\nsummary\" per finding)\n",
    "    # Step 2: Prompt the LLM to combine them into a well-structured markdown report that answers the original query\n",
    "    # Step 3: Return the report string\n",
    "    formatted_findings = \"\\n\\n\".join([f\"### {f['question']}\\n{f['summary']}\" for f in findings])\n",
    "    report_prompt = f\"Combine the following findings into a well-structured markdown report that answers the query: {query}\\n\\nFindings:\\n{formatted_findings}\"\n",
    "    report_resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=report_prompt,\n",
    "        temperature=1.2,\n",
    "    )\n",
    "    report = report_resp.output_text.strip()\n",
    "    return report\n",
    "\n",
    "\n",
    "def deep_research(query: str) -> str:\n",
    "    \"\"\"Run the full multi-agent deep research pipeline.\"\"\"\n",
    "    # Step 1: Call plan_research to break the query into sub-questions and print them\n",
    "    # Step 2: Use ThreadPoolExecutor to run search_and_summarize in parallel for each sub-question\n",
    "    # Step 3: Call synthesize_report to combine all findings into a final report\n",
    "    # Step 4: Return the report\n",
    "    sub_questions = plan_research(query)\n",
    "    print(\"Sub-questions:\")\n",
    "    for i, sq in enumerate(sub_questions, 1):\n",
    "        print(f\"{i}. {sq}\")\n",
    "\n",
    "    # Step 2: Use ThreadPoolExecutor to run search_and_summarize in parallel for each sub-question\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        findings = list(executor.map(search_and_summarize, sub_questions))\n",
    "\n",
    "    # Step 3: Call synthesize_report to combine all findings into a final report\n",
    "    report = synthesize_report(query, findings)\n",
    "    return report\n",
    "\n",
    "\n",
    "# Run the multi-agent research\n",
    "query = \"What are the best resources to learn machine learning in 2026?\"\n",
    "report = deep_research(query)\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have:\n",
    "* Practiced various inference-time reasoning methods (CoT, self-consistency, sequential revision, ToT)\n",
    "* Gained intuition about training reasoning models (STaR, ORM/PRM)\n",
    "* Built a **deep-research agent** with tool calling and ReAct-style reasoning\n",
    "* Implemented a **multi-agent system** with parallel research and report synthesis\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "deep_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
