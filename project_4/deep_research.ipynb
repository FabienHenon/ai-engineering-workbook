{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‚Äëof‚Äëthe‚Äëart inference‚Äëtime scaling methods such as *Chain‚Äëof‚ÄëThought* prompting and *Tree‚Äëof‚ÄëThoughts*, and briefly explore high-level concepts of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‚Äëtime scaling methods: **zero‚Äëshot / few‚Äëshot CoT, self‚Äëconsistency, sequential revision, tree‚Äëof‚Äëthoughts**  \n",
    "* Gain intuition for **training** reasoning‚Äëcapable models following **STaR** approach \n",
    "* Build a minimal **deep‚Äëresearch agent** that combines step‚Äëby‚Äëstep reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "0. Environment setup  \n",
    "1. Inference‚Äëtime scaling  \n",
    "  1.1 Few‚Äëshot.   \n",
    "  1.2 Zero‚Äëshot‚ÄØCoT.   \n",
    "  1.3 Self‚Äëconsistency.   \n",
    "  1.4 Sequential revisions.     \n",
    "  1.5 Tree‚Äëof‚ÄëThought (ToT)\n",
    "2. Training reasoning models and inspecting deepseek-r1 \n",
    "3. Deep-research agent  \n",
    "4. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "```\n",
    "\n",
    "#### Option 2: uv (Fast alternative)\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create a virtual environment and install dependencies\n",
    "uv venv .venv-deep-research && source .venv-deep-research/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Now open your notebook and switch to the `deep_research` kernel (Kernel ‚Üí Change Kernel).\n",
    "\n",
    "### Step 3: Setup and run Ollama serve\n",
    "\n",
    "In this project we use the `llama3.2:3b`, `qwen2.5:3b-instruct` and `deepseek-r1:1.5b` models. You can try other smaller or larger reasoning LLMs such as `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "Open terminal and run ollama:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "Then open another terminal and pull required models: \n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:1.5b\n",
    "ollama pull qwen2.5:3b-instruct\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull phi4-mini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 1‚Äë Inference‚Äëtime scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model‚Äôs weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we‚Äôll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 1.1: Few-Shot CoT\n",
    "\n",
    "Few-shot prompting provides examples before asking a new question. The model learns from the pattern and applies it to new inputs.\n",
    "\n",
    "We'll explore this with two models to understand how few-shot interacts with model capabilities:\n",
    "\n",
    "1. **GPT-2** (no instruction tuning): Doesn't reason by default. We'll see if few-shot examples can elicit reasoning.\n",
    "2. **Llama 3.2** (instruction-tuned): Already reasons naturally. We'll use few-shot to control the output format.\n",
    "\n",
    "#### GPT-2: Can few-shot examples elicit reasoning?\n",
    "\n",
    "GPT-2 is a base language model that just predicts the next token. It wasn't trained to follow instructions or reason step-by-step. Let's see what happens with and without few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1‚Äì2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate outputs with and without fewshot prompt and compare the difference.\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~12-15 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "#### Llama 3.2: Using few-shot to control output format\n",
    "\n",
    "Unlike GPT-2, Llama 3.2 is instruction-tuned and already produces reasoning traces by default. So what's the point of few-shot examples?\n",
    "\n",
    "**The power of few-shot with instruction-tuned models is controlling the output format.** We can make the model follow a specific structure like `[GIVEN]/[FIND]/[SOLVE]/[ANSWER]` that it wouldn't use naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# Step 1: Create your Ollama client\n",
    "# Step 2: Write a few examples showing reasoning steps\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer with and without few shot examples and compare them.\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 1.2: Zero‚ÄëShot Chain‚Äëof‚ÄëThought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as ‚ÄúLet‚Äôs think step by step.‚Äù This simple phrase often activates the model‚Äôs latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Create your Ollama client\n",
    "# Step 2: Write a question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 3: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~6 lines of code)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 1.3 Self‚ÄëConsistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def cot_answer(question, temperature=1.2):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def self_consistent(question, n=5):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~10 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter, traces = self_consistent(question, n=5)\n",
    "\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 1.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~20 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "# Step 3: Print the final output\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 1.5 Tree-of-Thoughts\n",
    "\n",
    "Tree-of-Thoughts (ToT) reframes reasoning as a search problem. Instead of generating one linear chain of thoughts, the model:\n",
    "1. Generates multiple candidate \"thoughts\" at each step\n",
    "2. Evaluates how promising each thought is\n",
    "3. Expands only the best candidates (beam search)\n",
    "4. Backtracks if needed\n",
    "\n",
    "This mirrors how humans solve hard problems: brainstorm options, evaluate them, pursue the best, and backtrack when stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpugbki39kv",
   "metadata": {},
   "source": [
    "#### Example 1: Word Ladder (Algorithmic ToT)\n",
    "\n",
    "This example shows ToT as pure beam search without LLM calls. Each \"thought\" is a candidate word that differs by one letter. We score by edit distance to goal and keep the best candidates.\n",
    "\n",
    "This demonstrates the **core algorithm** behind ToT: expand, score, prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6-8 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~14-18 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k84w1nz2u6p",
   "metadata": {},
   "source": [
    "#### Example 2: Generic ToT for Open-Ended Problems\n",
    "\n",
    "For open-ended problems without verifiable answers, we can still apply ToT by having the LLM both propose and evaluate thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067302",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next ‚Äúthoughts‚Äù that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client. Then return a list of stripped strings.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1‚Äì10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1‚Äì10;\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~5-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12-16 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‚Äë Training Models for Reasoning\n",
    "\n",
    "### 2.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 2.2: ORM¬†vs¬†PRM¬†+ RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‚Äëtuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‚Äëout (actions + log‚Äëprobs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e7757",
   "metadata": {},
   "source": [
    "### 2.3 Inspect a reasoning model\n",
    "\n",
    "Now that we've discussed how reasoning models are trained, let's see one in action. We'll use **DeepSeek-R1**, a reasoning model that produces explicit *thinking tokens* before giving its final answer. The model wraps its internal chain-of-thought inside `<think>...</think>` tags, followed by a clean final response.\n",
    "\n",
    "In the cell below we send a question to DeepSeek-R1 and parse the output to separate:\n",
    "- **Thinking tokens** ‚Äî the model's internal reasoning process (hidden from the end user in production).\n",
    "- **Final answer** ‚Äî the polished response the user actually sees.\n",
    "\n",
    "We use `deepseek-r1:1.5b` here for speed. You can switch to `deepseek-r1:8b` for higher-quality reasoning, but it will take longer to run. Pull whichever variant you want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Create OpenAI client and set your DeepSeek Model\n",
    "# Step 2: Write a math question\n",
    "# Step 3: Call your model\n",
    "# Step 4: Inspect the output. Separate thinking and final answer sections and print them.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~15 lines)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‚Äë A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with an LLM in a multi-step setup. We follow the *ReAct* pattern (reason ‚Üí tool ‚Üí observation):\n",
    "\n",
    "1. The model reasons and decides to use tools\n",
    "2. The agent searches and feeds condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `create_agent` from `langchain.agents`, which builds a ReAct-style agent graph. Note: the agent model must support **tool calling** (e.g., `llama3.2:3b`). Models like `deepseek-r1` are reasoning models that do not support native tool calling and cannot be used directly as the agent LLM. We can stick to the `llama3.2:3b` or `qwen2.5:3b-instruct` for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~3 lines of code)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL = \"qwen2.5:3b-instruct\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the LLM via ChatOllama (must support tool calling)\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Build a tool-calling agent with DuckDuckGo search\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "\"\"\"\n",
    "YOUR CODE HERE (2 line of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# 4- (Optional) Multi-Agent Deep Research\n",
    "\n",
    "Instead of a single agent, we can design multiple collaborating agents that work in parallel:\n",
    "\n",
    "1. **Planner**: Analyzes the query and breaks it into sub-questions\n",
    "2. **Researchers**: Run in parallel, each searching and summarizing findings for one sub-question  \n",
    "3. **Synthesizer**: Combines all research into a coherent final report\n",
    "\n",
    "This setup improves coverage and speed by parallelizing the research phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI\n",
    "from ddgs import DDGS\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def plan_research(query: str) -> list[str]:\n",
    "    \"\"\"Planner agent: breaks query into sub-questions.\"\"\"\n",
    "    # Prompt the LLM to decompose the query into 1-5 focused sub-questions.\n",
    "    # The prompt should instruct the model to return only sub-questions, one per line.\n",
    "    # Parse the response into a list of stripped strings and return\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def search_and_summarize(sub_question: str) -> dict:\n",
    "    \"\"\"Researcher agent: searches web and summarizes findings for one sub-question.\"\"\"\n",
    "    # Step 1: Use DDGS to search the web for the sub-question (max_results=3)\n",
    "    # Step 2: Join the result snippets into a single string\n",
    "    # Step 3: Prompt the LLM to write a concise summary based on the snippets\n",
    "    # Step 4: Return a dict with keys \"question\" and \"summary\"\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def synthesize_report(query: str, findings: list[dict]) -> str:\n",
    "    \"\"\"Synthesizer agent: combines all findings into a coherent report.\"\"\"\n",
    "    # Step 1: Format the findings list into a readable text block (e.g., \"### sub-question\\nsummary\" per finding)\n",
    "    # Step 2: Prompt the LLM to combine them into a well-structured markdown report that answers the original query\n",
    "    # Step 3: Return the report string\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~10 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def deep_research(query: str) -> str:\n",
    "    \"\"\"Run the full multi-agent deep research pipeline.\"\"\"\n",
    "    # Step 1: Call plan_research to break the query into sub-questions and print them\n",
    "    # Step 2: Use ThreadPoolExecutor to run search_and_summarize in parallel for each sub-question\n",
    "    # Step 3: Call synthesize_report to combine all findings into a final report\n",
    "    # Step 4: Return the report\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run the multi-agent research\n",
    "query = \"What are the best resources to learn machine learning in 2025?\"\n",
    "report = deep_research(query)\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have:\n",
    "* Practiced various inference-time reasoning methods (CoT, self-consistency, sequential revision, ToT)\n",
    "* Gained intuition about training reasoning models (STaR, ORM/PRM)\n",
    "* Built a **deep-research agent** with tool calling and ReAct-style reasoning\n",
    "* Implemented a **multi-agent system** with parallel research and report synthesis\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
